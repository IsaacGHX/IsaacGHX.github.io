[{"title":"解决CUDA10以后的GPU对于Tensorflow-1.x-gpu的依赖失败","url":"/2025/01/21/fast_startup/tf1_15_gpu_docker/","content":"写在前面工业界对于Tensorflow的喜爱是超乎我的意料的，可以说是因为TF1.x的运行前的静态编译使得对于代码和设备的优化能够更加细致，也可以说是大部分在21世纪10年代蓬勃发展的公司——尤其是中国的——都使用的是Nvidia的Volta架构，比如说V100/T4，这也可以说是在第一批贸易战禁令产物。\n其次是当年Meta(from 2021.10.28)还是Facebook，Google的TF有广大的市场，且启蒙我的《Deep Learning with Python》作者François Chollet领衔了Keras的集成进入TF. 以及轰轰烈烈的TF的2.x版本。\n彼时就觉得需要同时兼容pyhton+GPU编译+对应版本的TF库十分麻烦，尤其是对于初学者和便捷的Windows笔记本用户随时拉取一下github的项目测试一下，同时现在Docker越来越受到欢迎因此，姑且本文先通过Docker来省去一切的“为什么我的设备运行不了”的问题来列出解决步骤。\n废话就这么多，七步以内搞定。\n在Windows上使用Docker运行TensorFlow 1.15并启用GPU加速本指南以TF1.15为例，将帮助你在Windows上使用Docker运行TensorFlow 1.15，并充分利用你的NVIDIA GPU进行加速。将从安装Docker和NVIDIA支持开始，逐步引导完成整个过程。\n极简叙述\n安装Docker Desktop for Windows。\n安装NVIDIA GPU驱动程序，确保系统支持CUDA。\n拉取Docker镜像, &gt;&gt; docker pull tensorflow/tensorflow:1.15.0-gpu-py3\n启动Docker，&gt;&gt; docker run --gpus all -it --rm tensorflow/tensorflow:1.15.0-gpu-py3 bash\n确认Docker内CUDA和cuDNN，&gt;&gt; nvcc -V\n检查GPU可用， &gt;&gt; python -c “import tensorflow as tf; devices = tf.config.experimental.list_physical_devices(‘GPU’); print(devices)”\n输出为你的电脑的GPU数目即可。\n!!!该方法并不能够解决编译静态graph时候卡住的问题\n\n\n\n详细展开与特殊情况步骤1：安装Docker和NVIDIA Docker支持\n安装Docker Desktop for Windows\n\n下载并安装 Docker Desktop&lt;—点击进入，选择自己的机器对应的型号（通常Windows带了Nvidia的GPU的笔记本都是AMD64）。\n启动Docker并确保它在后台运行。docker --version\n输出理应格式如下：Docker version 20.10.17, build 100c701\n\n\n\n安装NVIDIA驱动程序和CUDA支持\n\n确保安装了最新版本的 NVIDIA GPU驱动程序&lt;—点击进入，选择自己的机器对应的型号。\\star只是为了能够运行查看当前的机器是不是有GPU！！如果没有请按照正常的tensorflow(默认cpu版本)安装就可以。在Windows本机环境下运行nvidia-smi\n输出理应格式如下：+-----------------------------------------------------------------------------------------+| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     ||-----------------------------------------+------------------------+----------------------+| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC || Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. ||                                         |                        |               MIG M. ||=========================================+========================+======================||   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A || N/A   49C    P4              8W /  109W |    2301MiB /   8188MiB |      2%      Default ||                                         |                        |                  N/A |+-----------------------------------------+------------------------+----------------------++-----------------------------------------------------------------------------------------+| Processes:                                                                              ||  GPU   GI   CI        PID   Type   Process name                              GPU Memory ||        ID   ID                                                               Usage      ||=========================================================================================||    0   N/A  N/A      ...    C+G   D:\\Microsoft VS Code\\Code.exe                N/A      ||    ...   N/A  N/A      ...    ...   ...      ...                                         |+-----------------------------------------------------------------------------------------+\n\n\n\n\n步骤2：拉取Docker镜像\n打开命令行工具并运行以下命令来获得docker镜像：\n docker pull tensorflow/tensorflow:1.15.0-gpu-py3\n 注意：其中的格式为docker pull tensorflow/tensorflow:1.xx-suffix_1-suffix_2 其中，tensorflow:1.xx-suffix_1，表示需要的版本，suffix_1一般添加gpu（不然你来看这干啥呢是吧） 其次，-suffix_2可以省略，由于其默认的下载是由Python2中的tensorflow包，是由Python2（e.g. 2.7.15+）版本编译的对于需要额外的Python3的语法项目存在部分冲突，所以建议添加此项为-py3。\n\n等待下载完成后，运行Docker的镜像：\n docker run --gpus all -it --rm tensorflow/tensorflow:1.15.0-gpu-py3 bash\n\n\n步骤3：拉取TensorFlow镜像拉取适用于Python 3环境的TensorFlow 1.15 GPU镜像：\ndocker pull tensorflow/tensorflow:1.15.0-gpu-py3\n输出大致为：________                               __________________  __/__________________________________  ____/__  /________      ____  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ //_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/WARNING: You are running this container as root, which can cause new files inmounted volumes to be created as the root user on your host machine.To avoid this, run the container by specifying your user&#x27;s userid:$ docker run -u $(id -u):$(id -g) args...root@container_id:/# **后续的所有的代码运行位置，输入Ctrl^D推出docker镜像这时候已经在镜像内了，一切格式都与Linux保持一致运行python -c &quot;import tensorflow as tf; devices = tf.config.experimental.list_physical_devices(&#x27;GPU&#x27;); print(devices)&quot;理应输出最后行格式如下：...blablabla2025-01-21 13:57:35.258965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0[PhysicalDevice(name=&#x27;/physical_device:GPU:0&#x27;, device_type=&#x27;GPU&#x27;)]gpu编号和列表编号一致从0开始，即说明成功啦！\n"},{"title":"Hello World","url":"/2025/01/18/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\nMore info: Writing\nRun server$ hexo server\nMore info: Server\nGenerate static files$ hexo generate\nMore info: Generating\nDeploy to remote sites$ hexo deploy\nMore info: Deployment\n"},{"title":"GNN系列1-GCN(Graph Convolutional Networks)","url":"/2025/01/23/insight/Graphs/Backbones/GCN/","content":"写在前面作为博客开篇，Graph系列的第一章，一切都只是我浅薄的观察和对于阅读的部分论文得出的见解，不能说是insight，只能够尽可能从我自己的武断认为其可能存在的道理来进行剖析，以及，我特别喜欢的，从发展中向后推进、以预判其潜在的改进空间。\n由于是第一章节，所以会牵扯到很多最最基础的定义，烦请耐心阅读。\n另外，就我对于工业界的实习来看，至少在2025年，当下的图结构数据的开发依然存在极其大的空间。在实际应用中发挥图结构的优势，最最重要的是建模，因为图不再是简单的离散点阵，更有图的结构——这也牵扯到什么是图什么是异构图。即，并非再是像朴素数据挖掘那样探讨如何处理图结构的数据来适应一个自回归任务，而是在于应当如何去构建图结构的数据，去定义什么是边，什么是节点，甚至隐式地定义图。一个非常简单的例子，如何教会机器人通过有限旋转关节的机械臂，来叠衣服，如果说这个方法可以通过将空间和衣服本身建模用点来进行建模的话。也许很多人会有和我一样的疑惑：是否图结构的数据在很多的算法中其实都有所崭露，而只是被其他角度的解读所掩盖了。\nGNN作为一个受到MLP和CV中的卷积神经网络的启发，而从深度学习的角度来建模图结构数据的将其作为一个从图数据到目标域的映射，这般的类推是十分朴实的，但是不可避免的需要“具体问题具体分析”——不是一个通用的网络就能够在所有的图相关任务中都达到最好的效果，就像是大语言模型需要在目标域进行微调一样。\n好，那么下面开始。\n参考原文：(GCN-3 KipfNet)Semi-Supervised Classification with Graph Convolutional Networks ICLR2017 | Paper | Cite代码: torch_geometric.nn.models.GCN\nI. 图结构数据定义常见的定义有二\n\n定义一：基于几何结构数学表示\n\n\\mathcal{G} = (\\mathcal{V},\\mathcal{E})一个图 (\\mathcal{G}) 由以下组成： \n\n节点集合 (\\mathcal{V}): 表示图中的所有节点，通常用 (|\\mathcal{V}| = N) 表示节点的数量。\n边集合 (\\mathcal{E}): 表示节点之间的连接关系，每条边由一对节点 ((u, v) \\in \\mathcal{V} \\times \\mathcal{V}) 表示。 \n节点特征矩阵 (\\mathrm{X}):\\mathrm{X} \\in \\mathbb{R}^{N \\times F}，其中F表示每个节点的特征维度。 \n几何结构(可选): 边的几何信息，如在二维或三维空间中的边的长度、方向等。\n\n形式化表示： \n\n \\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, \\mathrm{X}, \\text{几何信息})示例： \n\n\\mathcal{V} = \\{v_1, v_2, v_3\\}；\n\\mathcal{E} = \\{(v_1, v_2), (v_2, v_3)\\}；\n\\mathrm{X}=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}；\n边的几何长度：d_{12} = 1.5, d_{23} = 2.0。\n\n\n定义二：基于邻接矩阵数学表示：\n\n\\mathcal{G} = (\\mathrm{A}, \\mathrm{X}, \\mathrm{E})一个图\\mathcal{G}可以用以下方式定义：\n\n邻接矩阵 \\mathrm{A}: 表示图的拓扑结构，是一个\\mathrm{A} \\in \\mathbb{R}^{N \\times N}的稀疏矩阵（一般来说），其中：\n\\mathrm{A}_{ij} = 1表示节点i和j之间存在边；\n\\mathrm{A}_{ij} = 0表示节点i和j之间不存在边。\n\n\n节点特征矩阵 \\mathrm{X}: 表示每个节点的特征，是一个\\mathrm{X} \\in \\mathbb{R}^{N \\times F}，其中F表示每个节点的特征维度。\n边特征矩阵 \\mathrm{E}（可选）: 表示边的特征，是一个\\mathrm{E} \\in \\mathbb{R}^{|\\mathcal{E}| \\times F_e}，其中F_e表示每条边的特征维度。\n\n形式化表示：\n\n\\mathcal{G} = (\\mathrm{A}, \\mathrm{X}, \\mathrm{E})示例\n\\mathrm{A} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}；\n\n\\mathrm{X} = \\begin{bmatrix} 1 & 2 \\\\ 0 & 3 \\\\ 1 & 0 \\end{bmatrix}；\n\n\\mathrm{E} = \\begin{bmatrix} 0.5 \\\\ 0.8 \\\\ 1.2 \\end{bmatrix}（如边的权重或距离信息 ）。\n\n\n\n定义一关注几何结构，常用于需要明确空间信息的问题，如三维物体重建、分子建模和机器人路径规划，解决几何形状分析、分子性能预测和空间导航等问题；定义二更加关注节点信息和节点间的信息关系，广泛应用于社交网络、推荐系统和知识图谱，主要解决关系建模、个性化推荐以及复杂网络中信息传播与预测的问题。\n当然了在现在的很多工业应用当中抛弃显式的图结构，用简单的神经网络和序列建模Transformer类似的交叉注意力结构也依然能够得到一个非常好的效果，因为大部分工业应用中更加重视速度——尽管这个速度可以是通过学术界新创新方法之后不断地优化得到的。\nII. GCN原理图卷积网络可以分解为层的堆叠，其思路非常简单可以概括为：\n加权平均聚合 \\rightarrow 维度变换 \\rightarrow 非线性激活数学形式如下：\n\n\\mathrm{H}^{(l+1)} = \\sigma\\left( \\tilde{D}^{-\\frac{1}{2}} \\tilde{\\mathrm{A}} \\tilde{D}^{-\\frac{1}{2}} \\mathrm{H}^{(l)} W^{(l)} \\right) \\tag{2.1}\n\\tilde{\\mathrm{A}} = \\mathrm{A} + I：带自环的邻接矩阵\n\\tilde{D}：\\tilde{A}的度矩阵，\\tilde{D} ^{-\\frac{1}{2}}意味着对于其中所有的非零元素都取正平方根的倒数；\n入度矩阵（有向图，指向该节点的边）：D^{\\text{in}}_{ii} = \\sum_j A_{ji}，\n出度矩阵（有向图，从该节点指出的边）：D^{\\text{out}}_{ii} = \\sum_j A_{ij}，\n\n\nH^{(l)}：第l层的节点特征， H^{(0)} = \\mathrm{X}；\nW^{(l)}：可训练权重矩阵；\n\\sigma：激活函数（如ReLU）。\n\nIII. 不同角度的Rationale可以说，这是通过推广卷积和其上的拉普拉斯算子到图上，来将其变换到频域来进行计算，【Xavier Bresson教授图神经网络系列讲座_bilibili，图神经网络理论基础-人大-魏哲巍_bilibili】这是合理的。也可以从深度学习的角度出发（当然这也是深度卷积网络），所谓浅层表示local，深层表示global信息，即低频率特征值和高频特征值。\n\n关注II部分中的公式.(2.1)中的部分：\n\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)}本质上就是一个对于邻居节点和自身的加权平均，堆叠GNN的层可以看作是一个信息聚合的过程，如下图所示（有颜色的即为经过这次聚合后能够得到的信息）：\n\n这样的邻居聚合后共享参数其实蕴含了三个假设，\n\n第一，邻居节点和该节点的分类结果是相关的，这一假设与马尔科夫性质（Markov Property）相关，即一个节点的状态只依赖于其邻居节点的状态，也就会引出一些有关于马尔科夫毯（Markov blanket）的讨论，同时也引导人们看向诱导子图（Induced Graph）。\n第二，如果构建的是一个深度GNN，节点间的最短距离和相关性呈反比，即图信号处理中的平滑性假设（Smoothness Assumption）：图信号在局部区域内变化较小，也就是假设图信号的主要信息集中在低频部分，即图信号在局部区域内变化较小；\n第三，假设节点的特征和边的权重是独立的，即边的权重只反映节点之间的连接强度，而不直接依赖于节点特征，这也是为什么GCN原文用的是在社交图上的半监督的节点分类问题。可以说这三个假设是使得GCN这样的模式能够发挥作用的事后解释；也可以说是在发现其存在的问题之后的回顾，对于问题暂且先按下不表，先来看为什么他会有用，以及，是如何设计得更加符合神经网络工学的直觉的。\n\n这和图像卷积中的每次一个滑窗聚合其中的k\\times k的窗口内的像素点；抑或是信号与系统中的空域信号作倒序卷积，都有异曲同工之妙。\n重新定义图卷积（Graph Convolution）回顾一下卷积定义，其中t是连续时域变量，\\tau是窗口大小。\n(f * g)(t):= \\int_{\\infty}f( \\tau )g(t- \\tau )d \\tau离散版本，m 是窗口大小，n 是点列变量（离散时间），n\\in \\mathbb{Z}：\n(f*g)[n]=  \\sum_{m=-\\infty }^ {\\infty}f[m]g[n-m]=  \\sum _ {m=-\\infty }^ {\\infty }  f[n-m]g[m]卷积只是一种频域的分析方法，用滤波器g来筛选原来信号函数f的性质，那么，如何定义Graph上的卷积呢，我们只关注其中的任意一点n，观察对于它卷积的时候发生了什么：假设我们有以下输入信号 f 和卷积核 g：\n\n输入信号 f_n=[1,2,3,4]，索引范围为 0≤m≤3；\n卷积核 g_n=[1,2,1]，索引范围为 0≤m≤2。首先，将卷积核 g 翻转：g_{-m}=[1,2,1]，我们取其中非零的点来计算：\n当 n=3：\n卷积核覆盖的范围：n−m=3⇒m=1,2,3。\n有效范围：m=1,2,3。\n计算得到(f∗g)[3]=f[1]⋅g[2]+f[2]⋅g[1]+f[3]⋅g[0]=2⋅1+3⋅2+4⋅1=12即\\sum^{3}_{m=0} f[m] \\odot g_{-m}[m]，其中\\odot为哈德玛积，即逐个位置元素相乘。\n\n\n\n\n其上得到的是最后整个卷积后得到的函数的其中一个单位点，那么如果我们如果需要在图上定义什么是卷积，就需要定义什么是窗口？以及如何计算整个窗口内的各个元素的相加？\n对于单个节点，他的相邻的数据，显然，就是其相邻的节点，他们靠边来连接，因此很自然地，图卷积的定义就是\n\\tilde{A}\\mathrm{X}就这么简单。\n\n举栗.e.g.将矩阵可视化就是（假设是一个无向的图）：\n\n\n\n\n\\tilde{A}\n节点1\n节点2\n节点3\n节点4\n…\n\n\\mathrm{X}\n节点属性1\n属性2\n…\n\n\n\n\n节点1\n1\n0\n1\n0\n\n\n节点1\nblabla\nlbaba\nlabda\n\n\n节点2\n0\n1\n0\n0\n\n\\cdot\n节点2\nblala\nlbaba\n…\n\n\n节点3\n1\n0\n1\n1\n\n\n节点3\nblaa\nlba\n\n\n\n节点4\n0\n0\n1\n1\n\n\n节点4\nbabla\nlbdab\n\n\n\n…\n\n\n\n\n…\n\n…\n\n\n…\n\n\n\n\n将这个\\tilde{A} \\in \\mathbb{R}^{N \\times N}和\\mathrm{X} \\in \\mathbb{R}^{N \\times F}矩阵点积后可以得到更新后的\\mathrm{X}' \\in \\mathbb{R}^{N \\times F}\n其中（比如第一个节点的更新后的第一个属性，关注红色的行和列哈德玛积后相加），\n\\forall x'_i \\in \\mathrm{X}', x_j \\in N(j),x'_i = \\sum_{j} x_j.这里N(j)表示和原图中的节点x_j的1-hop邻居节点。\n\n那具体的步骤解释了，如何形式分析和定义呢？指路这家：图神经网络(GNN)入门之旅(三)-拉普拉斯矩阵与GCN - 知乎 (zhihu.com)；（看里面怎么定义拉普拉斯的就够了嗷，不要贪杯= ^ =）。简述就是希望模仿二维离散的拉普拉斯算子（如下），来推广图上的卷积和频谱分析：\n\\begin{pmatrix}  \n0 & 1  & 0  \\\\\n1 & -4  & 1 \\\\\n0 & 1  & 0 \\\\\n\\end{pmatrix}最最重要的是以下的这句话：\n“”假设具有 N 个节点的图 \\mathcal{G} ，此时图中每个节点的自由度至多为 N ，此时该图为完全图，即任意两个节点之间都有一条边连接，则对其中一个节点进行微扰，它可能变为图中任意一个节点。 \n此时以上定义的函数 f 不再是二维，而是 N 维向量： f=(f_1,f_2,...,f_N) ，其中 f_i 为函数 f 在图中节点 x_i 处的函数值。类比于二维函数 f(x,y) 在节点 (x,y) 处的值。 \n对 i 节点进行扰动，它可能变为任意一个与它相邻的节点 j∈N(i) , N(i) 表示节点 i 的一阶邻域节点。 \n我们上面已经知道拉普拉斯算子可以计算一个点到它所有自由度上微小扰动的增益，则通过图来表示就是任意一个节点 j 变化到节点 i 所带来的增益……“”\n也就是：图卷积是定义在图的边结构上的卷积，而 x_i \\sim p(X)，所有的节点都是来自于一个分布的采样；这其实也和其他的离散的信号定义保持了一致，但是其奥妙在于他是基于边的结构的卷积。\n\n但是显然地，我们会发现一个问题，在CV中，假设从函数的角度出发来看张量的变化，CNN(\\cdot) := f(x), x \\in \\mathbb{R}^{H \\times W \\times C} ，其输入和输出的张量形状是不一致的，即x^{(k)}= f(x^{(k-1)}), k≥1 的时候 (H^k \\times W^k) \\propto \\frac{1}{k} ，也就是如果不padding的话，CNN的张量随着网络深度的增加是越来越小的。但是GNN并非如此，GNN(\\cdot) := f(x), x \\in \\mathbb{R}^{N \\times F}, x^{(k)}= f(x^{(k-1)}), k≥1 的时候， N^k = N^{k-1},始终都是节点的个数，那自然会出现问题：也就是（当然并没有那么显然哦，只是我说的）GCN会遇到的，过渡平滑。\nIV. 图信号过渡平滑拉普拉斯平滑原理在图信号处理中，过渡平滑可通过图拉普拉斯矩阵量化，GCN传播一次的数学表达式为：\n\n\\mathcal{S}(f) = \\frac{1}{2} \\sum_{i,j=1}^n A_{ij} \\| f_i - f_j \\|^2 = f^\\top L f\nf_i, f_j，就是上述的Section.III的最后的引言部分的定义，对于每个节点的变换后的值；\nf \\in \\mathbb{R}^n，图信号向量；\nL = D - A， （姑且这样定义，也可以是归一化后的）组合拉普拉斯矩阵。因为拉普拉斯矩阵就是来刻画局部的平滑度，详见图像中运用了拉普拉斯核后的图像。\n\n平滑传播过程在图卷积网络中，多层传播导致的平滑效应可表示为：\n\nH^{(k+1)} = P H^{(k)} \\quad \\text{其中} \\quad P = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}对传播矩阵P（因为假设是拉普拉斯矩阵）进行特征分解：  \nP = U \\Lambda U^\\top经过K次传播后：  \nH^{(K)} = P^K H^{(0)} = U \\Lambda^K U^\\top H^{(0)}平滑动态特性那么自然地，从单个特征值的角度来看：\n\n高频分量衰减：\\lambda_i^K \\rightarrow 0 \\ (\\text{当}\\ |\\lambda_i| < 1)；\n低频分量保留：\\lambda_i^K \\rightarrow 1 \\ (\\text{当}\\ \\lambda_i \\approx 1)。\n\n过度平滑的数学描述当传播次数K \\rightarrow \\infty时，信号收敛至：  \n\\lim_{K \\to \\infty} P^K = \\frac{\\phi \\phi^\\top}{\\|\\phi\\|^2}其中\\phi是P的主特征向量，导致节点特征趋同：  \nH^{(\\infty)} \\propto \\phi \\cdot \\text{const}或者这样表示：\n\n设 h_i^{(k)} 表示节点 i 在第 k 层的特征；\n如果对于任意两个节点 i 和 j，有：\\lim_{k \\to \\infty} \\|h_i^{(k)} - h_j^{(k)}\\| = 0或者通过方差来刻画： \\text{Var}(H^{(k)}) \\to 0 \\quad \\text{当} \\quad k \\to \\infty\n\nV. 小结Kipf的GCN其实不是GCN的最初的频谱分析的流派的继承，而是做出了简单而且大胆的创新，虽然一开始是无向图，但是可以推广到有向图；\n其次是它的计算本质上是可以很快的，因为一切都取决于GPU上对于矩阵的点积操作的优化，但是，在大规模图上他的内存复杂消耗会到 O(N^2) 因为相当于直接存入邻接矩阵，而且稀疏图一般都是用链表或者是字典结构数据来存储的，因此不同存储的转化也存在加速的可能性；\n模型不具有随机性，没有独属于Graph的结构的数据增强和适合图的随机性对于方差的扩充。也就是，模型的泛化性不足 ———— 在OOD和存在分布偏移的数据上的泛化能力不足、对于动态图的适应能力较差。可以说这是由于他是transductive（直推）的，但是我不喜欢这种说辞，因为后续所谓提出inductive（归纳）方式解决了这个问题的GraphSAGE，实际上在动态图或者是分布差异的数据上的表现也不佳。\n虽然这也是其他所有的想要作更好的representation learning的模型的通病，这也有待后续的更多博客来探讨。博客文笔见疏，诸君评论多加指正。\n"},{"title":"吐槽一下概率论的定义符号","url":"/2025/01/31/insight/prob_base/","content":"写在前面恕我愚笨，在第一次学概率论的时候一直搞不清楚基础的定义，似乎后来所有的公式都是或多或少靠背诵的；重新回来复习的时候看到了基础的条件概率的定义，实在是觉得反直觉，因此用我自己喜欢的方式重写一下，方便日后速查。\n条件概率就是为了理解什么是条件概率，常见的条件概率的定义是这样的：\nP(B|A) = \\frac{P(A,B)}{P(A)}\\tag{1}意味着，在发生事件A的条件下，事件B发生的概率。\n令人迷惑的是什么是“在发生事件A的条件下”，而且后面会讨论到什么是不发生这件事情的概率，因此所以我喜欢先修改事件的定义为 A_i \\in {A}, i=0,1,...,|A| ，B_j \\in {B}, j=0,1,...,|B|，这样便于观察两个不同的事件组中的所有的事件的交叉可能性，而不再是一个事件发生xx情况的可能性。\n这样定义两个事件的条件概率，是最好理解的(因为存在B的时候A不能单独存在)：\nP(B_j|A_i) = \\frac{P(A_i,B_j)}{P(A_i, B)} \\tag{2}简单解释就是，“在A_i事件发生的条件下，在B事件族中发生B_j事件的可能性”。\n贝叶斯公式贝叶斯公式巧妙地联结了逆序的因果，如果说条件概率：P(B_j|A_i) 是 P(果|因) 的话，那么贝叶斯就是找到了 P(果|因) = Bayes(P(因|果))，也就是说，条件和结果是可互换的。\n常见的写法是：\nP(B|A) = \\frac{P(B)P(A|B)}{P(A)} = \\frac{P(B)P(A|B)}{P(B)P(A|B) + P(¬ {B})P(A|¬ B)} \\tag{3}但是这无论是顺序还是其中的定义符号，都太反化简约掉和对于事件的定义的直觉了，而且让我感觉困惑，因此按照公式(2)，可以改写成：\nP(B_j|A_i) = \\frac{P(A_i,B_j)}{P(A_i, B)} = \\frac{P(A_i|B_j)P(B_j)}{P(A_i|B_j)P(B_j) + P(A_i|B \\backslash B_j)P(B \\backslash B_j)} \\tag{4}这样一切都很顺眼了，分子就是公式(2)得到，分母就是：\nP(A_i, B) = P(A_i, B_j) + P(A_i, B \\backslash B_j)这样的形式其实也更好地能够引出为什么香农(Claude Elwood Shannon)会用log来定义信息熵，因为对数函数就是具有这样的性质：\nlogB + log(B/B_j) = log(B/B_j\\times B_j) = logB例子说服我自己，也说服你，试试看呢说不定就更有道理，哈哈哈哈。\n栗子1\n事件族 \\{A_i\\} ：明天下雨的情况\n\nA_0：不下雨\nA_1：下雨\n\n\n事件族 \\{B_j\\} ：某学生明天去上学的情况\n\nB_0：不上学\nB_1：上学\n\n\n\n我们可以用联合概率 P(A_i, B_j) 来表示两个事件同时发生的概率。根据你提供的表格，我们有：\n\n\n\n\nP(A_i, B_j)\nB_0(不上学)\nB_1(上学)\n\n\n\n\nA_0(不下雨)\n1/3\n1/3\n\n\nA_1(下雨)\n1/12\n1/4\n\n\n\n\n重新表述问题假设我们想要计算在某个特定条件下（例如，给定 A_i ）事件 B_j 发生的概率，即条件概率 P(B_j | A_i)。根据贝叶斯公式，我们可以这样计算：\nP(B_j | A_i) = \\frac{P(A_i, B_j)}{P(A_i,B)}其中：\n\nP(A_i, B_j) 是联合概率，即事件 A_i 和事件 B_j 同时发生的概率，\\sum P(A_i, B_j) =1。\nP(A_i, B) 是事件 A_i 发生的边缘概率。\n\n\n\n\n\nA\nA_0(不下雨)\nA_1(下雨)\n\n\n\n\nP(A_i)\n2/3\n1/3\n\n\n\n\n\nP(B_j, A) 是事件 B_j 发生的边缘概率。\n\n\n\n\n\nB\nB_0(不上学)\nB_1(上学)\n\n\n\n\nP(B_j)\n5/12\n7/12\n\n\n\n\n先有联合概率才有边缘概率分布！！！除非两个事件族独立\n\n示例计算\n计算 P(B_1 | A_1)（即在下雨的情况下学生上学的概率）：\nP(B_1 | A_1) = \\frac{P(A_1, B_1)}{P(A_1, B)}从表格中可以看到：\nP(A_1, B_1) = \\frac{1}{4}P(A_1, B) = P(A_1, B_0) + P(A_1, B_1) = \\frac{1}{12} + \\frac{1}{4} = \\frac{1}{3}因此：\nP(B_1 | A_1) = \\frac{\\frac{1}{4}}{\\frac{1}{3}} = \\frac{3}{4}\n\n\n栗子2癌症检测 —— 检测结果是阳性为事件A，实际患有癌症为事件C， 该医院检测的可靠度为95%（即患有癌症检测为阳性的概率为95%，没有癌症检测结果为阴性的概率为 95%），人群中患有癌症的概率为1%。求若检测结果为阳性，实际患有癌症的概率是多少。\n重新表述问题好的，我们可以通过贝叶斯公式来解决这个问题。假设：\n\n事件族 \\{A_i\\}：检测结果\n\nA_0：阴性\nA_1：阳性\n\n\n事件族 \\{C_j\\}：实际患有癌症情况\n\nC_0：没有患癌\nC_1：确实患癌\n\n\n\n已知条件如下：\n\n检测的可靠度为95%，即 P(A_1|C_1) = 0.95（患有癌症的情况下被检测为阳性）。\n同时，没患有癌症的情况下被检测为阴性的概率也为95%，因此P(A_0|C_0) = 0.95。\n上述二式可以得到误检率是5%：P(A_1∣¬C_1) = P(A_0∣¬C_0) = P(A_1∣C_0) = P(A_0∣C_1) =1−0.95=0.05\n人群中患有癌症的概率为1%，即 P(C_1) = 0.01。\n因此，没有癌症的概率为 P(C_0) = 1 - P(C) = 0.99。\n\n示例计算我们需要计算的是在检测结果为阳性的情况下，实际患有癌症的概率 P(C_1|A_1) ，即倒置因果，贝叶斯。\nP(C_1|A_1) = \\frac{P(C_1, A_1)}{P(C_1,A)} = \\frac{P(A_1 | C_1)P(C_1)}{P(A_1 | C_1)P(C_1) + P(A_1 | C_0)P(C_0)} \\approx 0.161\n或者，我们还有其他的解释词：\n将癌症检测问题转化为机器学习中的混淆矩阵，可以帮助我们更直观地理解模型的预测结果与实际情况之间的关系。混淆矩阵是一个特定格式的表格，用于描述分类模型（或“分类器”）的表现，显示了每个类别被正确和错误分类的情况。\n在你提供的癌症检测问题中，我们可以将其视为一个二分类问题，其中：\n\n正类（Positive, P）：实际患有癌症。\n负类（Negative, N）：实际上没有癌症。\n\n基于这些定义，我们可以构建如下的混淆矩阵（Confusion Matrix）：\n\n\n\n\n\n预测: 患有癌症 (阳性)\n预测: 未患癌症 (阴性)\n\n\n\n\n实际: 患有癌症\n真阳性 (TP)\n假阴性 (FN)\n\n\n实际: 未患癌症\n假阳性 (FP)\n真阴性 (TN)\n\n\n\n\n\n真阳性 (TP)：实际上患有癌症且被正确诊断为阳性的概率是 P(A_1 | C_1)P(C_1) = 0.95 \\times 0.01 = 0.0095 。\n假阴性 (FN)：实际上患有癌症但被错误地诊断为阴性的概率是 1 - TP = 0.05 \\times 0.01 = 0.0005 。\n假阳性 (FP)：实际上未患癌症但被错误地诊断为阳性的概率是 P(A_1 | C_0)P(C_0) = 0.05 \\times 0.99 = 0.0495 。\n真阴性 (TN)：实际上未患癌症且被正确诊断为阴性的概率是 1 - FP = 0.95 \\times 0.99 = 0.9405 。\n\n因此，淆矩阵如下所示：\n\n\n\n\n\n预测: 患有癌症 (阳性)\n预测: 未患癌症 (阴性)\n\n\n\n\n实际: 患有癌症\n0.95%\n0.05%\n\n\n实际: 未患癌症\n4.95%\n94.05%\n\n\n\n\n通过混淆矩阵，我们可以计算各种性能指标来评估分类器的效果，比如准确率（Accuracy）、精确率（Precision）、召回率（Recall），以及F1分数（F1 Score）。这些指标帮助我们全面了解分类模型的表现。\n其实原题就是求精确率\n计算公式1. 准确率（Accuracy）准确率是指所有预测正确的样本占总样本数的比例。\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}代入具体数值：\n\\text{Accuracy} = \\frac{0.0095 + 0.9405}{0.0095 + 0.9405 + 0.0495 + 0.0005} = \\frac{0.95}{1} = 0.952. 精确率（Precision）精确率是指被预测为正类的样本中实际为正类的比例。\n\\text{Precision} = \\frac{TP}{TP + FP}代入具体数值：\n\\text{Precision} = \\frac{0.0095}{0.0095 + 0.0495} = \\frac{0.0095}{0.059} \\approx 0.1613. 召回率（Recall）召回率是指实际为正类的样本中被正确预测为正类的比例。\n\\text{Recall} = \\frac{TP}{TP + FN}代入具体数值：\n\\text{Recall} = \\frac{0.0095}{0.0095 + 0.0005} = \\frac{0.0095}{0.01} = 0.954. F1 分数（F1 Score）F1分数是精确率和召回率的调和平均值，提供了单一指标来评估模型的整体表现。\n\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}代入精确率和召回率的具体数值：\n\\text{F1 Score} = 2 \\cdot \\frac{0.161 \\cdot 0.95}{0.161 + 0.95} \\approx 2 \\cdot \\frac{0.15295}{1.111} \\approx 0.275总结\n准确率 (Accuracy): 0.95 或者 95%\n精确率 (Precision): 0.161 或者 16.1%\n召回率 (Recall): 0.95 或者 95%\nF1 分数 (F1 Score): 0.275 或者 27.5%\n\n这些指标展示了模型在不同方面的表现：\n\n虽然整体准确率很高（95%），但这是因为大多数样本都是负类（未患癌症）。\n精确率较低（16.1%），意味着在所有被诊断为阳性的病例中，只有大约16.1%确实是患有癌症的。\n召回率较高（95%），说明大部分实际患有癌症的人都能被正确诊断出来。\nF1分数综合考虑了精确率和召回率，反映了模型在这两个方面的平衡情况。\n\n"}]