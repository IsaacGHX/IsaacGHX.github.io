<!DOCTYPE html><html lang="en-us" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Brief Reinforcement Learning 01 - Proximal Policy Optimization (PPO) 简单理解近端策略优化 | Isaac's Blog</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="preload" as="font" crossorigin="anonymous" href="/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/BenderLight.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
</style><script>var config = {"root":"/","search":{"preload":false,"activeHolder":"Enter here","blurHolder":"Search","noResult":"Data \"$0\" not found"},"code":{"codeInfo":"$0 - $1 lines","copy":"copy"}}</script><link type="text/css" rel="stylesheet" href="/lib/encrypt/hbe.style.css"><script src="/js/gitalk.js"></script><script src="//unpkg.com/mermaid@10.5.0/dist/mermaid.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
 menuSettings: {
   zoom: "None"
 },
 showMathMenu: false,
 jax: ["input/TeX","output/CommonHTML"],
 extensions: ["tex2jax.js"],
 TeX: {
   extensions: ["AMSmath.js","AMSsymbols.js"],
   equationNumbers: {
     autoNumber: "AMS"
   }
 },
 tex2jax: {
   inlineMath: [["\\(", "\\)"]],
   displayMath: [["\\[", "\\]"]]
 }
});</script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.woff2") format('woff2');
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
 --dark-background: url('/img/building_night.png');
 --light-background: url('/img/building_bright.png');
 --theme-encrypt-confirm: 'confirm'
}</style><script defer src="/js/arknights.js"></script><script defer src="/js/search.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script async src="/js/gitalk.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script><script>MathJax.Hub.Config({
  menuSettings: {
    zoom: "None"
  },
  showMathMenu: false,
  jax: ["input/TeX","output/CommonHTML"],
  extensions: ["tex2jax.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js"],
    equationNumbers: {
      autoNumber: "AMS"
    }
  },
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]]
  }
});
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/lib/encrypt/hbe.js"></script><script async src="/js/pjax.js"></script><script class="pjax-js">reset= () => {gitalk = new Gitalk({
 clientID: 'Ov23lio2QITWez44KoAo',
 clientSecret: '447a665fc75c690be677605bfd0210a4e15396aa',
 repo: 'Comment4io',
 owner: 'IsaacGHX',
 admin: ['IsaacGHX'],
 distractionFreeMode: false,
 id: location.pathname
});
if (document.querySelector("#gitalk")) gitalk.render("gitalk");document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.busuanzi'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);document.addEventListener('pjax:success', _ => bszCaller.fetch(
 "//busuanzi.ibruce.info/busuanzi?jsonpCallback=BusuanziCallback", a => {
  bszTag.texts(a),
  bszTag.shows()
}));reset()})</script><script class="pjax-js">reset= () => {gitalk = new Gitalk({
 clientID: 'Ov23lio2QITWez44KoAo',
 clientSecret: '447a665fc75c690be677605bfd0210a4e15396aa',
 repo: 'Comment4io',
 owner: 'IsaacGHX',
 admin: ['IsaacGHX'],
 distractionFreeMode: false,
 id: location.pathname
});
if (document.querySelector("#gitalk")) gitalk.render("gitalk");document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="Search" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem"><a class="navBlock" href="/about/"><span class="navItemTitle">About</span></a></li><li class="navItem"><a class="navBlock" href="/contact/"><span class="navItemTitle">Contact</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>Brief Reinforcement Learning 01 - Proximal Policy Optimization (PPO) 简单理解近端策略优化</h1><div id="post-info"><span>First Post: <div class="control"><time datetime="2025-07-30T15:12:47.000Z" id="date"> 2025-07-30</time></div></span><br><span>Last Update: <div class="control"><time datetime="2025-08-21T10:22:00.819Z" id="updated"> 2025-08-21</time></div></span><br><span>Word Count: <div class="control">5.6k</div></span><br><span>Read Time: <div class="control">22 min</div></span><br><span id="busuanzi_container_page_pv">Page View: <span class="control" id="busuanzi_value_page_pv">loading...</span></span></div></div><hr><div id="post-content"><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>PPO 原文: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a><br>TRPO 原文: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a></p>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ol>
<li><a href="#强化学习基础概念">强化学习基础概念</a><ul>
<li><a href="#策略-Policy">策略 (Policy)</a></li>
<li><a href="#动作-Action">动作 (Action)</a></li>
<li><a href="#奖励-Reward">奖励 (Reward)</a></li>
<li><a href="#轨迹-Trajectory">轨迹 (Trajectory)</a></li>
<li><a href="#价值函数-Value-Function-与-Critic">价值函数 (Value Function) 与 Critic</a></li>
<li><a href="#演员-actor">演员 (Actor)</a></li>
<li><a href="#优势函数-advantage-function">优势函数 (Advantage Function)</a></li>
</ul>
</li>
<li><a href="#从-Q-Learning-到-Policy-Gradient">从 Q-Learning 到 Policy Gradient</a></li>
<li><a href="#PPO-是什么？">PPO 是什么？</a><ul>
<li><a href="#PPO-流程示意图">PPO 流程示意图</a></li>
<li><a href="#PPO-的核心思想">PPO 的核心思想</a></li>
<li><a href="#PPO-的目标函数-Objective-Function">PPO 的目标函数 (Objective Function)</a></li>
</ul>
</li>
<li><a href="#PPO-算法详解">PPO 算法详解</a><ul>
<li><a href="#Clipped-Surrogate-Objective-Function">Clipped Surrogate Objective Function</a></li>
<li><a href="#价值函数损失-Value-Function-Loss">价值函数损失 (Value Function Loss)</a></li>
<li><a href="#多轮次更新-Multiple-Epochs-of-Minibatch-Updates">多轮次更新 (Multiple Epochs of Minibatch Updates)</a></li>
<li><a href="#PPO-算法流程">PPO 算法流程</a></li>
</ul>
</li>
<li><a href="#实例：用-PPO-玩石头剪刀布">实例：用 PPO 玩转石头剪刀布</a><ul>
<li><a href="#环境设定">环境设定</a></li>
<li><a href="#模型设计">模型设计</a></li>
<li><a href="#训练过程">训练过程</a></li>
<li><a href="#结果">结果</a></li>
</ul>
</li>
<li><a href="#附录：数学推导">附录：数学推导</a><ul>
<li><a href="#策略梯度定理推导">策略梯度定理推导</a></li>
<li><a href="#信赖域方法推导">信赖域方法推导</a></li>
</ul>
</li>
</ol>
<hr>
<h1 id="强化学习基础概念"><a href="#强化学习基础概念" class="headerlink" title="强化学习基础概念"></a>强化学习基础概念</h1><p>在深入 PPO 之前，我们首先需要理解一些强化学习（Reinforcement Learning, RL）的基本构建块。想象一个智能体（Agent）在一个环境（Environment）中学习，它通过不断地试错来最大化自己获得的奖励。</p>
<ul>
<li><p><strong>策略 (Policy)</strong>: <script type="math/tex">\pi</script><br>策略是智能体的大脑，它定义了智能体在特定状态下 <script type="math/tex">(s)</script> 会采取什么动作 <script type="math/tex">(a)</script>。策略可以有两种形式：</p>
<ul>
<li><strong>确定性策略 (Deterministic Policy)</strong>: <script type="math/tex">a = μ(s)</script>，在每个状态下，动作是确定的。</li>
<li><strong>随机性策略 (Stochastic Policy)</strong>: <script type="math/tex">π(a|s) = P(A_t = a | S_t = s)</script>，在每个状态下，策略会给出一个采取各个动作的概率分布。PPO 处理的是随机性策略。</li>
</ul>
</li>
<li><p><strong>动作 (Action)</strong>: <script type="math/tex">a</script><br>智能体根据其策略在环境中执行的操作。例如，在游戏中是按下某个按钮，在机器人控制中是移动某个关节。</p>
</li>
<li><p><strong>奖励 (Reward)</strong>: <script type="math/tex">r</script><br>当智能体执行一个动作后，环境会反馈一个标量信号——奖励。这个信号评价了这个动作的好坏。智能体的最终目标是最大化累积奖励（Cumulative Reward）。</p>
</li>
<li><p><strong>轨迹 (Trajectory)</strong>: <script type="math/tex">\tau</script><br>智能体与环境交互产生的一系列状态、动作和奖励的序列，可以表示为 <script type="math/tex">\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ...)</script>。</p>
</li>
<li><p><strong>价值函数 (Value Function) 与 Critic</strong><br>价值函数用来评估一个状态或一个状态-动作对的“好坏”程度，即从该点出发，预期未来能获得多少总奖励。</p>
<ul>
<li><strong>状态价值函数 (State-Value Function)</strong> <script type="math/tex">V^\pi(s)</script>: 从状态 <script type="math/tex">s</script> 出发，遵循策略 <script type="math/tex">\pi</script>，所能获得的期望总回报。</li>
<li><strong>动作价值函数 (Action-Value Function)</strong> <script type="math/tex">Q^\pi(s, a)</script>: 在状态 <script type="math/tex">s</script> 下，执行动作 <script type="math/tex">a</script>，然后遵循策略 <script type="math/tex">\pi</script>，所能获得的期望总回报。<br>在 Actor-Critic 架构中，<strong>Critic（评论家）</strong> 的角色就是学习并输出价值函数，它的作用是“评价”当前 Actor 的表现好坏，但它自己不决定做什么动作。</li>
</ul>
</li>
<li><p><strong>演员 (Actor)</strong><br><strong>Actor（演员）</strong> 的角色是学习并执行策略 <script type="math/tex">\pi</script>。它根据当前状态 <script type="math/tex">s</script>，决定要采取哪个动作 <script type="math/tex">a</script>。Actor 的目标是调整策略，以获得更高的总回报。</p>
</li>
<li><p><strong>优势函数 (Advantage Function)</strong>: <script type="math/tex">A^\pi(s, a)</script><br>优势函数是衡量在状态 <script type="math/tex">s</script> 下，采取动作 <script type="math/tex">a</script> 相对于遵循当前策略 <script type="math/tex">\pi</script> 的平均表现有多好。它的计算公式是：</p>
<script type="math/tex; mode=display">A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)</script><ul>
<li>如果 <script type="math/tex">A > 0</script>, 说明动作 <script type="math/tex">a</script> 比平均水平要好。</li>
<li>如果 <script type="math/tex">A < 0</script>, 说明动作 <script type="math/tex">a</script> 比平均水平要差。</li>
</ul>
<p><strong><em> 优势函数是 PPO 算法中的一个核心概念，它告诉我们策略更新应该朝哪个方向进行。</em></strong></p>
</li>
</ul>
<h1 id="从-Q-Learning-到-Policy-Gradient"><a href="#从-Q-Learning-到-Policy-Gradient" class="headerlink" title="从 Q-Learning 到 Policy Gradient"></a>从 Q-Learning 到 Policy Gradient</h1><p>传统的 <strong>Q-Learning</strong> 是一种基于价值的方法。它通过学习一个最优的动作价值函数 <script type="math/tex">Q^*(s, a)</script> 来间接得到最优策略。其策略通常是贪婪的：在状态 <script type="math/tex">s</script> 下，选择使 <script type="math/tex">Q(s, a)</script> 值最大的动作 <script type="math/tex">a</script>。这种方法在处理连续动作空间或需要随机策略时会遇到困难。</p>
<p>为了解决这些问题，<strong>策略梯度 (Policy Gradient, PG)</strong> 方法应运而生。PG 不再学习价值函数，而是直接对策略 <script type="math/tex">\pi_\theta(a|s)</script> 进行参数化（<script type="math/tex">\theta</script> 是神经网络的参数），然后通过梯度上升来优化策略，以最大化期望总回报 <script type="math/tex">\mathcal{J}(\theta)</script>。</p>
<p>PG 的核心思想很简单：如果一个动作带来了好的结果（即高的优势值），我们就增加这个动作被选择的概率；反之，则减少。</p>
<p>然而，朴素的 PG 算法存在一些问题：</p>
<ol>
<li><strong>高方差</strong>：梯度的估计可能非常不稳定，导致训练过程震荡。</li>
<li><strong>更新步长难以确定</strong>：如果更新步长（学习率）太大，可能会导致策略“崩溃”，即更新后的策略表现急剧下降，且难以恢复。如果步长太小，则训练速度过慢。</li>
</ol>
<h1 id="PPO-是什么？"><a href="#PPO-是什么？" class="headerlink" title="PPO 是什么？"></a>PPO 是什么？</h1><p><strong>近端策略优化 (Proximal Policy Optimization, PPO)</strong> 是一种旨在解决策略梯度方法中更新步长问题的算法。它是对 <strong>信赖域策略优化 (Trust Region Policy Optimization, TRPO)</strong> 的一种简化，在实现上更简单，但效果同样出色。</p>
<h2 id="PPO-流程示意图"><a href="#PPO-流程示意图" class="headerlink" title="PPO 流程示意图"></a>PPO 流程示意图</h2><p>下面是一个简化的 PPO 工作流程图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs mermaid">graph TD<br>    A[Actor: π_θ_old] -- &quot;与环境交互&quot; --&gt; B(收集轨迹 τ);<br>    B -- &quot;计算每个时间步的优势 A_t&quot; --&gt; C;<br>    C[Critic: V_φ] -- &quot;计算V(s_t)辅助计算A_t&quot; --&gt; B;<br>    B -- &quot;将(s_t, a_t, A_t)存入缓冲区&quot; --&gt; D(经验缓冲区);<br>    D -- &quot;重复K个Epoch&quot; --&gt; E&#123;优化循环&#125;;<br>    E -- &quot;采样一个Minibatch&quot; --&gt; F(计算PPO目标函数 L_CLIP);<br>    F -- &quot;计算梯度 ∇_θ L_CLIP&quot; --&gt; G(更新Actor网络参数 θ);<br>    E -- &quot;采样一个Minibatch&quot; --&gt; H(计算价值损失 L_VF);<br>    H -- &quot;计算梯度 ∇_φ L_VF&quot; --&gt; I(更新Critic网络参数 φ);<br>    G &amp; I -- &quot;K个Epoch结束后&quot; --&gt; J[新策略 π_θ];<br>    J -- &quot;π_θ_old ← π_θ&quot; --&gt; A;<br></code></pre></td></tr></table></figure>
<h2 id="PPO-的核心思想"><a href="#PPO-的核心思想" class="headerlink" title="PPO 的核心思想"></a>PPO 的核心思想</h2><p>PPO 的核心思想是：<strong>在尝试最大化目标函数的同时，使用一个“惩罚”项来限制新旧策略之间的差异，确保每次更新不会让策略变得太离谱。</strong></p>
<p>想象一下你在一个山坡上试图走到山顶（最大化奖励）。朴素的 PG 方法就像是你蒙着眼睛朝你认为最陡峭的方向迈出一大步，但你可能会不小心滚下悬崖（策略崩溃）。PPO 则是在你脚上绑了一根绳子，绳子的另一端固定在你之前的位置。你可以自由地向任何方向迈步，但如果步子迈得太大，绳子就会把你拉回来，防止你摔得太远。</p>
<p>这个“绳子”就是 PPO 中的 <strong>Clipping (裁剪)</strong> 机制。</p>
<h2 id="PPO-的目标函数-Objective-Function"><a href="#PPO-的目标函数-Objective-Function" class="headerlink" title="PPO 的目标函数 (Objective Function)"></a>PPO 的目标函数 (Objective Function)</h2><p>PPO 的目标函数是其精髓所在，我们先来看最常用的 <strong>PPO-Clip</strong> 的目标函数：</p>
<script type="math/tex; mode=display">\mathcal{L}^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]</script><script type="math/tex; mode=display">r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}</script><p>让我们来拆解这个复杂的公式：</p>
<ul>
<li><script type="math/tex">\hat{\mathbb{E}}_t[...]</script>: 表示对一个批次（batch）中所有时间步 <script type="math/tex">t</script> 的样本取平均。</li>
<li><script type="math/tex">r_t(\theta)</script>: 这是新旧策略之间的概率比率 (probability ratio)。<ul>
<li><script type="math/tex">\pi_\theta(a_t | s_t)</script>: 当前正在优化的新策略。</li>
<li><script type="math/tex">\pi_{\theta_{\text{old}}}(a_t | s_t)</script>: 用于收集数据的旧策略。</li>
<li>如果 <script type="math/tex">r_t(\theta) > 1</script>, 说明新策略更倾向于采取动作 <script type="math/tex">a_t</script>。</li>
<li>如果 <script type="math/tex">r_t(\theta) < 1</script>, 说明新策略不太倾向于采取动作 <script type="math/tex">a_t</script>。</li>
</ul>
</li>
<li><script type="math/tex">\hat{A}_t</script>: 这是在时间步 <script type="math/tex">t</script> 的优势函数 <code>Advantage</code> 的估计值。</li>
<li><script type="math/tex">\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon)</script> : 这个函数将概率比率 <script type="math/tex">r_t(\theta)</script> 裁剪到一个范围 <script type="math/tex">[1 - \epsilon, 1 + \epsilon]</script> 内。<script type="math/tex">\epsilon</script> 是一个超参数，通常取 <code>0.1</code> 或 <code>0.2</code>。</li>
<li><script type="math/tex">\min(...)</script>: PPO 的关键部分。它在两个项之间取最小值。</li>
</ul>
<h1 id="PPO-算法详解"><a href="#PPO-算法详解" class="headerlink" title="PPO 算法详解"></a>PPO 算法详解</h1><h2 id="Clipped-Surrogate-Objective-Function"><a href="#Clipped-Surrogate-Objective-Function" class="headerlink" title="Clipped Surrogate Objective Function"></a>Clipped Surrogate Objective Function</h2><p>我们来详细分析 <script type="math/tex">\min</script> 函数中的两项：</p>
<ol>
<li><p><strong><script type="math/tex">r_t(\theta) \hat{A}_t</script></strong>: 这是标准的策略梯度目标函数。如果优势 <script type="math/tex">\hat{A}_t</script> 是正的，我们会想要增大 <script type="math/tex">r_t(\theta)</script>（即增加 <script type="math/tex">\pi_\theta(a_t|s_t)</script>），从而最大化这一项。如果 <script type="math/tex">\hat{A}_t</script> 是负的，我们会想要减小 <script type="math/tex">r_t(\theta)</script>。</p>
</li>
<li><p><strong><script type="math/tex">\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t</script></strong>: 这是被裁剪过的版本，是 PPO 的创新之处。</p>
</li>
</ol>
<p><strong>为什么要取 <script type="math/tex">\min</script>？</strong></p>
<p>这是一种悲观主义的或者说保守的更新方式。</p>
<ul>
<li><p><strong>当 <script type="math/tex">\hat{A}_t > 0</script> (好动作) 时:</strong></p>
<script type="math/tex; mode=display">\mathcal{L}^{CLIP} = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta), 1 + \epsilon) \hat{A}_t \right]</script><p>这意味着，我们希望增加好动作的概率 (<script type="math/tex">r_t(\theta)</script> 变大)，但是这个增加是有限度的。<script type="math/tex">r_t(\theta)</script> 最多只能增长到 <script type="math/tex">1+\epsilon</script>。这防止了策略因为一个特别好的动作而过度更新，导致在其他状态下表现变差。</p>
</li>
<li><p><strong>当 <script type="math/tex">\hat{A}_t < 0</script> (坏动作) 时:</strong></p>
<script type="math/tex; mode=display">\mathcal{L}^{CLIP} = \hat{\mathbb{E}}_t \left[ \max(r_t(\theta), 1 - \epsilon) \hat{A}_t \right]</script><p>(注意，因为 <script type="math/tex">\hat{A}_t</script> 是负数，<script type="math/tex">\min</script> 实际上变成了 <script type="math/tex">\max</script>）。<br>这意味着，我们希望减小坏动作的概率 (<script type="math/tex">r_t(\theta)</script> 变小)，但这个减小也是有限度的。<script type="math/tex">r_t(\theta)</script> 最少只能减小到 <script type="math/tex">1-\epsilon</script>。这防止了策略因为一个坏动作而过度惩罚，导致策略完全放弃探索某些可能在未来有价值的动作。</p>
</li>
</ul>
<p>通过这种方式，PPO 将策略更新限制在了一个“信赖域”内，使得训练过程更加稳定。</p>
<h2 id="价值函数损失-Value-Function-Loss"><a href="#价值函数损失-Value-Function-Loss" class="headerlink" title="价值函数损失 (Value Function Loss)"></a>价值函数损失 (Value Function Loss)</h2><p>在 PPO 这种 Actor-Critic 架构中，除了负责决策的 Actor (策略网络)，还有一个负责“评价”的 <strong>Critic (价值网络)</strong>。Critic 的作用是学习状态价值函数 <script type="math/tex">V(s)</script>，即评估处于某个状态 <script type="math/tex">s</script> 下有多好。这个评估值对于计算优势函数 <script type="math/tex">\hat{A}_t</script> 至关重要，因为 <script type="math/tex">\hat{A}_t</script> 直接指导了 Actor 的更新方向。</p>
<p>价值函数损失就是专门用来训练 Critic 网络的损失函数。它的目标是让 Critic 对状态价值的预测 <script type="math/tex">V_\phi(s_t)</script> (其中 <script type="math/tex">\phi</script> 是价值网络的参数) 尽可能地接近“真实”的价值。</p>
<p><strong>1. 公式定义</strong></p>
<p>价值函数损失通常是一个简单的均方误差 (Mean Squared Error, MSE)：</p>
<script type="math/tex; mode=display">\mathcal{L}^{VF}(\phi) = \hat{\mathbb{E}}_t \left[ (V_\phi(s_t) - V_t^{\text{target}})^2 \right]</script><p>让我们来解析这个公式：</p>
<ul>
<li><script type="math/tex">\hat{\mathbb{E}}_t[\dots]</script>: 表示对一个批次（batch）中所有时间步 <script type="math/tex">t</script> 的样本取平均。</li>
<li><script type="math/tex">V_\phi(s_t)</script>: 这是 Critic 网络对状态 <script type="math/tex">s_t</script> 的预测价值。</li>
<li><script type="math/tex">V_t^{\text{target}}</script>: 这是我们在该时间步观测到的“目标价值”或“真实价值”。它是一个我们希望 Critic 网络输出的目标。</li>
</ul>
<p><strong>2. 目标价值 <script type="math/tex">V_t^{\text{target}}</script> 是什么？</strong></p>
<p>既然 <script type="math/tex">V(s)</script> 本身就是对未来总回报的期望，那么最直接的“真实价值”就是我们在该轮游戏中，从状态 <script type="math/tex">s_t</script> 开始实际获得的<strong>累积回报</strong>（也称为 Monte Carlo Return）。</p>
<p>在实际操作中，<script type="math/tex">V_t^{\text{target}}</script> 通常就是我们计算 GAE (Generalized Advantage Estimation) 时用到的回报估计值。简单来说，它可以是：</p>
<script type="math/tex; mode=display">V_t^{\text{target}} = \hat{A}_t + V_\phi(s_t)</script><p>这个公式看起来可能有点循环引用，但在计算时，<script type="math/tex">V_\phi(s_t)</script> 是从旧的网络中得到的值（<code>value.detach()</code>），而 <script type="math/tex">\hat{A}_t</script> 是已经基于这批数据计算好的优势估计。所以，我们实际上是让新的价值网络去拟合一个更精确的、结合了实际奖励和旧价值估计的目标。</p>
<p><strong>3. 为什么需要这个损失项？</strong></p>
<p>训练 Critic 的目的就是为了给 Actor 提供一个准确的<strong>优势函数估计 <script type="math/tex">\hat{A}_t</script></strong>。</p>
<ul>
<li>一个<strong>准确的 Critic</strong> 能够提供低方差的优势估计，这使得 Actor 的更新更加稳定和高效。如果 Critic 对价值的评估是胡乱猜测的，那么计算出的优势信号也会充满噪声，误导 Actor 的学习方向。</li>
<li>通过最小化 <script type="math/tex">\mathcal{L}^{VF}</script>，我们不断地用实际观测到的回报来校准 Critic 的判断力，让它成为一个越来越可靠的“评论家”。</li>
</ul>
<p><strong>4. 在 PPO 中的作用</strong></p>
<p>在 PPO 的整体优化目标中，价值损失 <script type="math/tex">\mathcal{L}^{VF}</script> 是作为辅助损失项存在的。它与 Actor 的策略损失 <script type="math/tex">\mathcal{L}^{CLIP}</script> 结合在一起，形成一个总的损失函数：</p>
<script type="math/tex; mode=display">\mathcal{L}(\theta, \phi) = \mathcal{L}^{CLIP}(\theta) - c_1 \mathcal{L}^{VF}(\phi) + c_2 S[\pi_\theta](s_t)</script><p>其中 <script type="math/tex">c_1</script> 是价值损失的系数，用于平衡策略学习和价值学习的重要性。这两个网络通常会一起训练，但它们的目标不同：Actor (参数 <script type="math/tex">\theta</script>) 负责最大化 <script type="math/tex">\mathcal{L}^{CLIP}</script>，而 Critic (参数 <script type="math/tex">\phi</script>) 负责最小化 <script type="math/tex">\mathcal{L}^{VF}</script>。</p>
<h2 id="多轮次更新-Multiple-Epochs-of-Minibatch-Updates"><a href="#多轮次更新-Multiple-Epochs-of-Minibatch-Updates" class="headerlink" title="多轮次更新 (Multiple Epochs of Minibatch Updates)"></a>多轮次更新 (Multiple Epochs of Minibatch Updates)</h2><p>PPO 的另一个重要特点是它可以在同一批数据上进行多次（K个Epoch）的梯度更新。这大大提高了样本的利用效率。传统的 A2C (Advantage Actor-Critic) 算法每收集一批数据只能更新一次网络，而 PPO 可以用这批数据训练好几个 Epoch，只要策略更新不偏离旧策略太远（由 Clip 机制保证）。</p>
<h2 id="PPO-算法流程"><a href="#PPO-算法流程" class="headerlink" title="PPO 算法流程"></a>PPO 算法流程</h2><p>下面是 PPO 算法更具体的伪代码：</p>
<ol>
<li><strong>初始化</strong>: 初始化 Actor 网络 <script type="math/tex">\pi_\theta</script> 和 Critic 网络 <script type="math/tex">V_\phi</script> 的参数 <script type="math/tex">\theta, \phi</script>。</li>
<li><p><strong>循环</strong> (for iteration = 1, 2, …):<br>a.  <strong>数据收集</strong>: 使用当前策略 <script type="math/tex">\pi_{\theta_{\text{old}}} \leftarrow \pi_\theta</script>，与环境交互 <code>N</code> 个时间步，收集一批轨迹 <script type="math/tex">\mathcal{D} = \{\tau_i\}</script>。<br>b.  <strong>优势计算</strong>: 对收集到的每个时间步 <script type="math/tex">t</script>，计算优势函数 <script type="math/tex">\hat{A}_t</script>。通常使用 GAE (Generalized Advantage Estimation) 方法来平衡偏差和方差。</p>
<script type="math/tex; mode=display">\hat{A}_t = \delta_t + (\gamma\lambda)\delta_{t+1} + ... + (\gamma\lambda)^{T-t+1}\delta_{T-1}</script><p>其中 <script type="math/tex">\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)</script><br>c.  <strong>优化循环</strong> (for epoch = 1, 2, …, K):</p>
<ul>
<li>从 <script type="math/tex">\mathcal{D}</script> 中随机采样一个小批量（Minibatch）数据。</li>
<li>计算 Actor 的损失 <script type="math/tex">\mathcal{L}^{CLIP}(\theta)</script>。</li>
<li>计算 Critic 的损失 <script type="math/tex">\mathcal{L}^{VF}(\phi) = ||(V_\phi(s_t) - V_t^{\text{target}}||^2_2</script> (均方误差)。<script type="math/tex">V_t^{\text{target}}</script> 通常是 <script type="math/tex">\hat{A}_t + V_\phi(s_t)</script>。</li>
<li>（可选）计算熵损失 <script type="math/tex">S[\pi_\theta](s_t)</script>，鼓励探索。</li>
<li>更新 Actor 和 Critic 的网络参数：</li>
</ul>
<script type="math/tex; mode=display">\theta \leftarrow \theta - \alpha_\theta \nabla_\theta (\mathcal{L}^{CLIP} - c_1 \mathcal{L}^{VF} + c_2 S)</script><script type="math/tex; mode=display">\phi \leftarrow \phi - \alpha_\phi \nabla_\phi \mathcal{L}^{VF}</script></li>
<li><strong>结束</strong></li>
</ol>
<h1 id="实例：用-PPO-玩石头剪刀布"><a href="#实例：用-PPO-玩石头剪刀布" class="headerlink" title="实例：用 PPO 玩石头剪刀布"></a>实例：用 PPO 玩石头剪刀布</h1><p>让我们看一个简单的例子：训练一个 AI 来玩石头剪刀布。</p>
<h2 id="环境设定"><a href="#环境设定" class="headerlink" title="环境设定"></a>环境设定</h2><ul>
<li><strong>对手</strong>: 不是完全随机的，而是一个有特定偏好的对手。比如，他出“石头”的概率是 50%，出“剪刀”和“布”的概率各是 25%。</li>
<li><strong>状态 (State)</strong>: 为了简单起见，我们可以将状态设为对手上一次出的手势。如果游戏是独立的，状态也可以是一个常数。</li>
<li><strong>动作 (Action)</strong>: 我们的 AI 可以选择出“石头”(0)、“剪刀”(1) 或“布”(2)。</li>
<li><strong>奖励 (Reward)</strong>:<ul>
<li>赢: +1</li>
<li>平: 0</li>
<li>输: -1</li>
</ul>
</li>
</ul>
<h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><ul>
<li><strong>Actor</strong>: 一个简单的神经网络。输入是状态（对手上一次的手势，独热编码），输出是三个动作（石头、剪刀、布）的概率分布（通过 Softmax 层）。</li>
<li><strong>Critic</strong>: 另一个简单的神经网络。输入是状态，输出是一个标量，代表当前状态的价值 <script type="math/tex">V(s)</script>。</li>
</ul>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><ol>
<li><strong>初始化</strong>: 随机初始化 Actor 和 Critic 网络的权重。我们的 AI 一开始是胡乱出拳的。</li>
<li><strong>收集数据</strong>: 让我们的 AI (Actor) 和有偏好的对手玩，比如玩 100 局。记录下每一局的状态、我们出的动作、以及获得的回报。例如，记录 (<script type="math/tex">s_t</script>= <code>对手出石头</code> , <script type="math/tex">a_t</script>= <code>我出剪刀</code>, <script type="math/tex">r_t</script>= <code>-1</code>)。</li>
<li><strong>计算优势</strong>:<ul>
<li>用 Critic 网络预测每一局开始时的状态价值 <script type="math/tex">V(s_t)</script>。</li>
<li>因为石头剪刀布是单步游戏，优势函数可以简化为 <script type="math/tex">A_t = r_t - V(s_t)</script>。</li>
<li>例如，在 <script type="math/tex">s_t</script>（对手上把出石头）时，我们出了“布”赢了 (<script type="math/tex">r_t=1</script>)。假设 Critic 预测 <script type="math/tex">V(s_t)=0.1</script>。那么优势 <script type="math/tex">A_t = 1 - 0.1 = 0.9</script>。这是一个很大的正优势。</li>
</ul>
</li>
<li><strong>PPO 更新</strong>:<ul>
<li>我们使用收集到的 100 局数据，进行多轮（比如 K=4）优化。</li>
<li>在每一轮中，我们计算 PPO 的 <code>\mathcal&#123;L&#125;CLIP</code> 损失。</li>
<li>对于刚才那个例子，因为 <script type="math/tex">A_t=0.9</script> 是正的，算法会尝试提高在 <script type="math/tex">s_t</script> 状态下出“布”的概率。</li>
<li><code>clip</code> 机制会确保这个概率的提升不会太大，比如 <script type="math/tex">r_t</script> 不会超过 1.2。</li>
<li>同时，我们也更新 Critic 网络，让它的预测 <script type="math/tex">V(s_t)</script> 更接近实际获得的回报 <script type="math/tex">r_t</script>。</li>
</ul>
</li>
<li><strong>重复</strong>: 不断地重复步骤 2-4。</li>
</ol>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>经过多轮训练后：</p>
<ul>
<li><strong>Actor</strong> 会学到，当对手有 50% 的概率出“石头”时，我应该提高出“布”的概率，这样胜率最高。它的策略会逐渐收敛到一个最优解（高概率出布）。</li>
<li><strong>Critic</strong> 会学到，在面对这个对手时，游戏的初始状态价值是正的，因为我们有优势。</li>
</ul>
<p>这个例子展示了 PPO 如何通过与环境交互，稳定地学习到一个能利用环境特性（对手偏好）的策略。</p>
<hr>
<h1 id="附录：数学推导"><a href="#附录：数学推导" class="headerlink" title="附录：数学推导"></a>附录：数学推导</h1><p>这里提供了一些核心概念的数学推导，以供深入理解。</p>
<h2 id="策略梯度定理推导"><a href="#策略梯度定理推导" class="headerlink" title="[策略梯度定理推导]"></a>[策略梯度定理推导]</h2><p>策略梯度定理是策略梯度方法的基础，它表明了目标函数 <script type="math/tex">\mathcal{J}(\theta)</script> 的梯度可以被写成一个期望的形式，从而可以用蒙特卡洛采样来估计。我们的目标是找到能最大化期望总回报 <script type="math/tex">\mathcal{J}(\theta)</script> 的策略参数 <script type="math/tex">\theta</script>。</p>
<p><strong>1. 目标函数定义</strong></p>
<p>首先，我们定义目标函数 <script type="math/tex">\mathcal{J}(\theta)</script> 为遵循策略 <script type="math/tex">\pi_\theta</script> 时，所有可能轨迹 <script type="math/tex">\tau</script> 的期望总回报。</p>
<script type="math/tex; mode=display">\mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)] = \sum_{\tau} P(\tau|\theta) R(\tau)</script><p>其中，<script type="math/tex">R(\tau) = \sum_{t=0}^{T} r(s_t, a_t)</script> 是轨迹 <script type="math/tex">\tau</script> 的总回报，<script type="math/tex">P(\tau|\theta)</script> 是在参数为 <script type="math/tex">\theta</script> 的策略下，轨迹 <script type="math/tex">\tau</script> 发生的概率。</p>
<p><strong>2. 求目标函数的梯度</strong></p>
<p>我们对目标函数求关于 <script type="math/tex">\theta</script> 的梯度：</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{J}(\theta) = \nabla_\theta \sum_{\tau} P(\tau|\theta) R(\tau) = \sum_{\tau} \nabla_\theta P(\tau|\theta) R(\tau)</script><p><strong>3. 应用对数导数技巧 (Log-Derivative Trick)</strong></p>
<p>直接计算 <script type="math/tex">\nabla_\theta P(\tau|\theta)</script> 很困难。这里我们使用一个关键技巧：<script type="math/tex">\nabla_x f(x) = f(x) \nabla_x \log f(x)</script>。将其应用到 <script type="math/tex">P(\tau|\theta)</script> 上：</p>
<script type="math/tex; mode=display">\nabla_\theta P(\tau|\theta) = P(\tau|\theta) \nabla_\theta \log P(\tau|\theta)</script><p>将这个技巧代入梯度公式中：</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{J}(\theta) = \sum_{\tau} P(\tau|\theta) \nabla_\theta \log P(\tau|\theta) R(\tau)</script><p>这个形式正好是某个期望值的定义，所以可以写成：</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [\nabla_\theta \log P(\tau|\theta) R(\tau)]</script><p><strong>4. 展开轨迹概率</strong></p>
<p>现在我们来处理 <script type="math/tex">\log P(\tau|\theta)</script>。一条轨迹的概率是初始状态概率和一系列动作概率与状态转移概率的乘积：</p>
<script type="math/tex; mode=display">P(\tau|\theta) = p(s_0) \prod_{t=0}^{T} \pi_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)</script><p>对其取对数：</p>
<script type="math/tex; mode=display">\log P(\tau|\theta) = \log p(s_0) + \sum_{t=0}^{T} \left( \log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t, a_t) \right)</script><p>再对其求关于 <script type="math/tex">\theta</script> 的梯度。注意到，环境的状态转移概率 <script type="math/tex">p(s_{t+1}|s_t, a_t)</script> 和初始状态概率 <script type="math/tex">p(s_0)</script> 都与策略参数 <script type="math/tex">\theta</script> 无关，所以它们的梯度为零。因此：</p>
<script type="math/tex; mode=display">\nabla_\theta \log P(\tau|\theta) = \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)</script><p><strong>5. 得到策略梯度的基本形式</strong></p>
<p>将上式代入第3步的期望公式中，我们得到：</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \left( \sum_{t=0}^{T} r(s_t, a_t) \right) \right]</script><p>这个公式虽然正确，但方差很大，因为括号里的回报项 <script type="math/tex">R(\tau)</script> 会同时乘以过去和未来的所有动作的梯度。</p>
<p><strong>6. 利用因果关系并引入基线以减小方差</strong></p>
<p>一个重要的观察是：在时间步 <script type="math/tex">t</script> 的决策 <script type="math/tex">\pi_\theta(a_t|s_t)</script> 只会影响从 <script type="math/tex">t</script> 时刻开始的未来回报，而不会影响过去已经获得的回报。因此，我们可以将回报项替换为从当前时刻开始的未来回报总和 <script type="math/tex">G_t = \sum_{t'=t}^{T} r(s_{t'}, a_{t'})</script>：</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right]</script><p>为了进一步减小方差，我们可以从回报中减去一个不依赖于动作 <script type="math/tex">a_t</script> 的基线（baseline）<script type="math/tex">b(s_t)</script>。最常用的基线是状态价值函数 <script type="math/tex">V(s_t)</script>。减去基线不会改变梯度的期望值（因为 <script type="math/tex">\mathbb{E}[\nabla_\theta \log \pi_\theta(a_t|s_t) b(s_t)] = 0</script>），但可以显著减小梯度的方差。</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - V(s_t)) \right]</script><p><strong>7. 最终形式：优势函数</strong></p>
<p>我们发现，<script type="math/tex">G_t - V(s_t)</script> 正是优势函数 <script type="math/tex">A(s_t, a_t)</script> 的一个估计。因此，策略梯度定理最终可以写成我们熟悉的形式：</p>
<script type="math/tex; mode=display">\nabla_\theta \mathcal{J}(\theta) \approx \hat{\mathbb{E}}_t \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}_t \right]</script><p>这个形式直观地告诉我们：如果一个动作的优势 <script type="math/tex">\hat{A}_t</script> 是正的，我们就调整参数 <script type="math/tex">\theta</script> 来增加这个动作的对数概率 <script type="math/tex">\log \pi_\theta(a_t|s_t)</script>；反之亦然。这就是策略梯度方法的核心。</p>
<h2 id="信赖域方法推导"><a href="#信赖域方法推导" class="headerlink" title="[信赖域方法推导]"></a>[信赖域方法推导]</h2><p>TRPO (PPO 的前身) 的目标函数可以写成：</p>
<script type="math/tex; mode=display">\max_\theta \quad \mathbb{E}_{\substack{s \sim \rho^{\pi_{\theta_{old}}}， a \sim \pi_{\theta_{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a) \right]</script><script type="math/tex; mode=display">\text{s.t.} \quad \mathbb{E}_{s \sim \rho^{\pi_{\theta_{old}}}} \left[ D_{KL}(\pi_{\theta_{old}}(\cdot|s) || \pi_\theta(\cdot|s)) \right] \leq \delta</script><p>这里的 <script type="math/tex">D_{KL}</script> 是 KL 散度，用来衡量新旧策略的差异。这个约束确保了策略更新不会偏离旧策略太远，从而保证了训练的稳定性。然而，求解这个带约束的优化问题非常复杂，需要计算二阶导数（Hessian矩阵）。</p>
<p>PPO 通过 <code>clip</code> 函数来近似这个带约束的优化问题，将其转化为一个无约束的、更容易求解的优化问题。PPO 的目标函数可以看作是 TRPO 目标函数的一阶近似的、加了惩罚项的版本，从而大大简化了计算，同时保留了信赖域方法的稳定性。</p>
<div class="github-star-promo"><!-- [新] 现在只有一个 promo-text 的 div--><div class="promo-text">如果觉得有用欢迎给我的项目加一个 ⭐<!-- [新] iframe 直接放在文字中间--><iframe src="https://ghbtns.com/github-btn.html?user=IsaacGHX&amp;repo=IsaacGHX.github.io&amp;type=star&amp;count=true&amp;size=large" frameborder="0" scrolling="0" width="170" height="30" title="Star this project on GitHub"></iframe> =w=/</div></div><div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2025/08/05/fast_startup/%E8%B6%85%E5%B8%B8%E7%94%A8-Linux-%E5%91%BD%E4%BB%A4%E5%90%88%E8%BE%91/">← Next 超常用 Linux 命令合辑</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2025/07/27/fast_startup/Configure-Clash-Nyanpasu-on-Linux/">Configure Clash Nyanpasu on Linux Prev →</a></div></div></div><div id="comments"><div id="gitalk"></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="To Top" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="To Catalog">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="Change Theme"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/2.png" alt="Logo" style="margin:0;border-radius:0;"></a><h1 id="Dr"><a href="/about">Isaac IPF</a></h1><div id="description"><p></p></div></div><div id="aside-block"><div id="toc-div"><h1>Catalog</h1><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="toc-number">1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">2.</span> <span class="toc-text">目录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">3.</span> <span class="toc-text">强化学习基础概念</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8E-Q-Learning-%E5%88%B0-Policy-Gradient"><span class="toc-number">4.</span> <span class="toc-text">从 Q-Learning 到 Policy Gradient</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PPO-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">PPO 是什么？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO-%E6%B5%81%E7%A8%8B%E7%A4%BA%E6%84%8F%E5%9B%BE"><span class="toc-number">5.1.</span> <span class="toc-text">PPO 流程示意图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">5.2.</span> <span class="toc-text">PPO 的核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO-%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0-Objective-Function"><span class="toc-number">5.3.</span> <span class="toc-text">PPO 的目标函数 (Objective Function)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PPO-%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="toc-number">6.</span> <span class="toc-text">PPO 算法详解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Clipped-Surrogate-Objective-Function"><span class="toc-number">6.1.</span> <span class="toc-text">Clipped Surrogate Objective Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E6%8D%9F%E5%A4%B1-Value-Function-Loss"><span class="toc-number">6.2.</span> <span class="toc-text">价值函数损失 (Value Function Loss)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%BD%AE%E6%AC%A1%E6%9B%B4%E6%96%B0-Multiple-Epochs-of-Minibatch-Updates"><span class="toc-number">6.3.</span> <span class="toc-text">多轮次更新 (Multiple Epochs of Minibatch Updates)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">6.4.</span> <span class="toc-text">PPO 算法流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%94%A8-PPO-%E7%8E%A9%E7%9F%B3%E5%A4%B4%E5%89%AA%E5%88%80%E5%B8%83"><span class="toc-number">7.</span> <span class="toc-text">实例：用 PPO 玩石头剪刀布</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E8%AE%BE%E5%AE%9A"><span class="toc-number">7.1.</span> <span class="toc-text">环境设定</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="toc-number">7.2.</span> <span class="toc-text">模型设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">7.3.</span> <span class="toc-text">训练过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-number">7.4.</span> <span class="toc-text">结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC"><span class="toc-number">8.</span> <span class="toc-text">附录：数学推导</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86%E6%8E%A8%E5%AF%BC"><span class="toc-number">8.1.</span> <span class="toc-text">[策略梯度定理推导]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E8%B5%96%E5%9F%9F%E6%96%B9%E6%B3%95%E6%8E%A8%E5%AF%BC"><span class="toc-number">8.2.</span> <span class="toc-text">[信赖域方法推导]</span></a></li></ol></li></ol></div></div><footer><nobr>Published with <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> by <a target="_blank" rel="noopener" href="https://github.com/IsaacGHX">Isaac_GHX </a></nobr><wbr><nobr> Theme <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> by <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr><wbr></footer></aside></main><canvas id="canvas-dust"></canvas></body></html>